{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webis/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "FILE_TEST = 'formatted_csv_dataset/formatted_test.tweet.csv'\n",
    "FILE_TRAIN = 'formatted_csv_dataset/formatted_training.csv'\n",
    "\n",
    "test_tweets_df = pd.read_csv(FILE_TEST, sep=',')\n",
    "test_tweets_df = test_tweets_df[['text', 'coordinates_lat', 'coordinates_long', 'tweet_city']]    # Keep only relevant columns\n",
    "\n",
    "tweets_df = pd.read_csv(FILE_TRAIN, sep=',')\n",
    "\n",
    "# Swap latitude and longitude column values; we made a mistake when formatting the dataset to a csv file\n",
    "tweets_df.rename(columns={'coordinates_lat' : 'coordinates_lat_old'}, inplace=True)\n",
    "tweets_df.rename(columns={'coordinates_long' : 'coordinates_lat'}, inplace=True)\n",
    "tweets_df.rename(columns={'coordinates_lat_old' : 'coordinates_long'}, inplace=True)\n",
    "\n",
    "tweets_df = tweets_df[['id', 'text', 'coordinates_lat', 'coordinates_long', 'tweet_city']]    # Keep only relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(tweet):\n",
    "    if not tweet:\n",
    "        return ''\n",
    "    try:\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)    # Remove urls\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')    # Remove special characters\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "    except:\n",
    "        print('error: {}'.format(tweet))\n",
    "        return ''\n",
    "\n",
    "    if len(tokens) < 3:    # Ignore tweets with less than 3 words  \n",
    "        return ''\n",
    "        \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def get_preprocessed_df(df):\n",
    "    df['text'] = df['text'].apply(lambda text: preprocess(text))   \n",
    "    df = df.loc[df['text'].apply(lambda t: t.strip() != '')]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "print('Preprocessing text')\n",
    "tweets_df = get_preprocessed_df(tweets_df)\n",
    "test_tweets_df = get_preprocessed_df(test_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EMBEDDINGS = False    # Configure word embeddings based features or tf-idf based    \n",
    "NUM_FEATURES = 300\n",
    "\n",
    "\n",
    "def get_tweet_vector(tweet_row, model, vocab):\n",
    "    num_tokens = 1\n",
    "    tweet_vec = np.zeros(NUM_FEATURES)\n",
    "    \n",
    "    for i in range(len(vocab)):\n",
    "        token = vocab[i]\n",
    "        token_val = tweet_row[i]    # tf-idf value for feature corresponding to the token\n",
    "        \n",
    "        if token not in model.vocab or token_val == 0.0:\n",
    "            continue\n",
    "            \n",
    "        token_vec = model.get_vector(token)         \n",
    "        tweet_vec += token_val * token_vec\n",
    "        num_tokens += 1\n",
    "        \n",
    "    return tweet_vec / num_tokens\n",
    "            \n",
    "                    \n",
    "        \n",
    "def get_embedding_features(tf_idf_features, model, vocab):\n",
    "    embed_features = list()\n",
    "    \n",
    "    for tweet_row in tf_idf_features:\n",
    "        embed_features.append(get_tweet_vector(tweet_row, model, vocab))\n",
    "        \n",
    "    return np.array(embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = None\n",
    "\n",
    "if USE_EMBEDDINGS:\n",
    "    print('loading embedding model')\n",
    "    word_embeddings_file = '../../../text_rewriting/arnet_miner_analysis/wiki.en.vec'\n",
    "    model = KeyedVectors.load_word2vec_format(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features\n",
      "Training and predicting lat and long using classifier: GaussianNB()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def report_result(test_labels, predicted_labels):\n",
    "    print('Accuracy: ', accuracy_score(y_true=test_labels, y_pred=predicted_labels))\n",
    "    print('Macro f1 score: ', f1_score(y_true=test_labels, y_pred=predicted_labels, average='macro'))\n",
    "    print('Micro f1 score: ', f1_score(y_true=test_labels, y_pred=predicted_labels, average='micro'))\n",
    "\n",
    "\n",
    "TRAIN_SIZE = 100000\n",
    "\n",
    "train_df = tweets_df.head(TRAIN_SIZE)\n",
    "test_df = test_tweets_df\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')    # Stop words filtered\n",
    "\n",
    "print('Extracting tf-idf features')\n",
    "train_features = vectorizer.fit_transform(train_df['text']).toarray()    \n",
    "test_features = vectorizer.transform(test_df['text']).toarray()\n",
    "\n",
    "if USE_EMBEDDINGS:\n",
    "    print('Computing word vector based features')    \n",
    "    vocab = vectorizer.get_feature_names()    \n",
    "    assert len(vocab) == len(train_features[0])\n",
    "    assert len(vocab) == len(test_features[0])    \n",
    "    train_tf_idf_shape = train_features.shape\n",
    "    test_tf_idf_shape = test_features.shape\n",
    "    \n",
    "    train_features = get_embedding_features(train_features, model, vocab)\n",
    "    test_features = get_embedding_features(test_features, model, vocab)\n",
    "    \n",
    "    assert train_features.shape[0] == train_tf_idf_shape[0]\n",
    "    assert test_features.shape[0] == test_tf_idf_shape[0]\n",
    "    \n",
    "\n",
    "# Predicting latitude and longitude\n",
    "\n",
    "train_labels = train_df['tweet_city']\n",
    "\n",
    "# classifier = LogisticRegression()\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# classifier = SVC()\n",
    "\n",
    "print('Training and predicting city using classifier: {}'.format(classifier))\n",
    "classifier.fit(train_features, train_labels)\n",
    "test_df['predicted_city'] = classifier.predict(test_features)\n",
    "\n",
    "# We assign arbitrary values to coordinates since we're only interested in city prediction accuracy\n",
    "test_df['predicted_long'] = 0.0    \n",
    "test_df['predicted_lat'] = 0.0\n",
    "\n",
    "# report_result(test_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['id'] = test_df.index\n",
    "\n",
    "output_df = test_df[['id', 'predicted_city', 'predicted_lat', 'predicted_long']]\n",
    "oracle_df = test_df[['id', 'tweet_city', 'coordinates_lat', 'coordinates_long']]\n",
    "\n",
    "output_data = list(output_df.to_records(index=False))\n",
    "oracle_data = list(oracle_df.to_records(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating result\n",
      "WNUT evaluation: , , 0.0297, 7776.0522, 7405.35\n"
     ]
    }
   ],
   "source": [
    "# Official evaluation for the shared task - WNUT\n",
    "\n",
    "# !{sys.executable} -m pip install ujson\n",
    "\n",
    "# import ujson as json\n",
    "import math\n",
    "\n",
    "EARTH_RADIUS = 6372.8\n",
    "\n",
    "\n",
    "def _calc_dist_radian(pLat, pLon, lat, lon):\n",
    "    \"\"\"\n",
    "    Calculate the Great Circle Distance between two points on earth\n",
    "    http://en.wikipedia.org/wiki/Great-circle_distance\n",
    "    \"\"\"\n",
    "    cos_pLat = math.cos(pLat)\n",
    "    sin_pLat = math.sin(pLat)\n",
    "    cos_lat = math.cos(lat)\n",
    "    sin_lat = math.sin(lat)\n",
    "    long_diff = pLon - lon\n",
    "    cos_long_diff = math.cos(long_diff)\n",
    "    sin_long_diff = math.sin(long_diff)\n",
    "    numerator = math.sqrt(math.pow(cos_lat * sin_long_diff, 2) +\n",
    "                          math.pow(cos_pLat * sin_lat - sin_pLat * cos_lat * cos_long_diff, 2))\n",
    "    denominator = sin_pLat * sin_lat + cos_pLat * cos_lat * cos_long_diff\n",
    "    radian = math.atan2(numerator, denominator)\n",
    "    return radian * EARTH_RADIUS\n",
    "\n",
    "\n",
    "def _degree_radian(degree):\n",
    "    return (degree * math.pi) / 180\n",
    "\n",
    "\n",
    "def calc_dist_degree(pLat, pLon, lat, lon):\n",
    "    pLat = _degree_radian(pLat)\n",
    "    pLon = _degree_radian(pLon)\n",
    "    lat = _degree_radian(lat)\n",
    "    lon = _degree_radian(lon)\n",
    "    return _calc_dist_radian(pLat, pLon, lat, lon)\n",
    "\n",
    "\n",
    "def evaluate_submission(output_data, oracle_data):\n",
    "#     output_data = etl_data(output_file, submission_type)\n",
    "#     oracle_data = etl_data(oracle_file, submission_type)\n",
    "    assert len(output_data) == len(oracle_data)\n",
    "    accuracy, median_error, mean_error = 0.0, 0.0, 0.0\n",
    "    right, wrong = 0, 0\n",
    "    error_distance_list = []\n",
    "    for output, oracle in zip(output_data, oracle_data):        \n",
    "        assert output[0] == oracle[0]\n",
    "        if output[1] == oracle[1]:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        error_distance = calc_dist_degree(output[2], output[3], oracle[2], oracle[3])\n",
    "        error_distance_list.append(error_distance)\n",
    "\n",
    "    accuracy = round(right / (right + wrong + 1e-6), 4)\n",
    "    error_distance_list.sort()\n",
    "    total_num = len(error_distance_list)\n",
    "        \n",
    "    mean_error = round(sum(error_distance_list) / (total_num + 1e-6), 4)\n",
    "    median_error = round(error_distance_list[int(total_num / 2)], 4)\n",
    "    \n",
    "    \n",
    "    output_file = ''\n",
    "    submission_type = ''\n",
    "    \n",
    "    result = \"WNUT evaluation: {}, {}, {}, {}, {}\".format(output_file,\n",
    "                                                  submission_type,\n",
    "                                                  accuracy,\n",
    "                                                  median_error,\n",
    "                                                  mean_error)\n",
    "    print(result)\n",
    "    \n",
    "\n",
    "    \n",
    "print('Evaluating result')\n",
    "evaluate_submission(output_data, oracle_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print('Finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
