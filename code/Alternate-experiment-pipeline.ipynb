{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project - Heuristic and Learning based approaches for Twitter Geolocation\n",
    "### Notebook for Geolocation Prediction in Twitter â€“ SS 2020\n",
    "#### Vishal Khanna-Mat. 120333-CS4DM\n",
    "#### Sneha Mohanty-Mat. 120799-CS4DM\n",
    "#### Sagar Nagaraj SimhaMat. 120797, CS4DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate experimental setting with pipeline of Stemming, TFIDF vectorization and SCG Classifier with 100 clusters.\n",
    "#### Results: Accuracy - 12.04%, Median error distance - 1271km, Mean error distance - 4004km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Reading from the file and selecting the relevant columns\n",
    "FILE_TEST = 'formatted_test.tweet.csv'\n",
    "FILE_TRAIN = 'formatted_training.csv'\n",
    "\n",
    "test_tweets_df = pd.read_csv(FILE_TEST, sep=',')\n",
    "test_tweets_df = test_tweets_df[['text', 'coordinates_lat', 'coordinates_long', 'tweet_city']]    # Keep only relevant columns\n",
    "\n",
    "tweets_df = pd.read_csv(FILE_TRAIN, sep=',') # The training dataset\n",
    "\n",
    "# Swap latitude and longitude column values; we made a mistake when formatting the dataset to a csv file\n",
    "tweets_df.rename(columns={'coordinates_lat' : 'coordinates_lat_old'}, inplace=True)\n",
    "tweets_df.rename(columns={'coordinates_long' : 'coordinates_lat'}, inplace=True)\n",
    "tweets_df.rename(columns={'coordinates_lat_old' : 'coordinates_long'}, inplace=True)\n",
    "\n",
    "tweets_df = tweets_df[['id', 'text', 'coordinates_lat', 'coordinates_long', 'tweet_city', 'lang']]    # Keep only relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text\n",
      "error: nan\n",
      "error: nan\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing. Remove URLs and special characters and ignoring tweets lesser than 3 tokens\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(tweet):\n",
    "    if not tweet:\n",
    "        return ''\n",
    "    try:\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)    # Remove urls\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')    # Remove special characters\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "    except:\n",
    "        print('error: {}'.format(tweet))\n",
    "        return ''\n",
    "\n",
    "    if len(tokens) < 3:    # Ignore tweets with less than 3 words  \n",
    "        return ''\n",
    "        \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def get_preprocessed_df(df):\n",
    "    df['text'] = df['text'].apply(lambda text: preprocess(text))   \n",
    "    df = df.loc[df['text'].apply(lambda t: t.strip() != '')]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "print('Preprocessing text')\n",
    "tweets_df = get_preprocessed_df(tweets_df)\n",
    "test_tweets_df = get_preprocessed_df(test_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with 100 means\n"
     ]
    }
   ],
   "source": [
    "# Creating labels for classification by clustering of cordinate points-lat,long (setting num_clusters based on heuristics of number of cities/districts etc)\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "NUM_CLUSTERS = 100\n",
    "mbk = MiniBatchKMeans(init ='k-means++', n_clusters = NUM_CLUSTERS, \n",
    "                      batch_size = 200, \n",
    "                      max_no_improvement = 10, verbose = 0) \n",
    "\n",
    "points = list(zip(tweets_df.coordinates_lat, tweets_df.coordinates_long))\n",
    "coordinates_arr = np.array(points)\n",
    "#coordinates_arr = tweets_df[['coordinates_lat', 'coordinates_long']].values\n",
    "\n",
    "print('Clustering with {} means'.format(NUM_CLUSTERS))\n",
    "mbk.fit(coordinates_arr)\n",
    "tweets_df['cluster_label'] = mbk.labels_\n",
    "cluster_centers = mbk.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_label\n",
       "0      26485\n",
       "1     319661\n",
       "2     302348\n",
       "3     269345\n",
       "4      40248\n",
       "       ...  \n",
       "95     60601\n",
       "96      8064\n",
       "97     52009\n",
       "98      5984\n",
       "99     70102\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of datapoints in each cluster label. This is to show the skewed distribution of cities.\n",
    "tweets_df.groupby(['cluster_label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to blancing the dataset using Downsampler, Upsampler and SMOTE. Downsampler looses information.\n",
    "# SMOTE and oversamper requires online learning mechanism to run over large datasets.\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_res, y_res = rus.fit_resample(tweets_df, tweets_df['cluster_label'])\n",
    "# X_res.groupby(['cluster_label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline - Stemming, TFIDF vectorization and SGD Classifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Stemming Code\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "# text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer(smooth_idf=True,use_idf=True)), \n",
    "#                              ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer(smooth_idf=True,use_idf=True)), \n",
    "                             ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(tweets_df['text'], tweets_df['cluster_label'])\n",
    "\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(test_tweets_df['text'])\n",
    "\n",
    "#np.mean(predicted_mnb_stemmed == test_tweets_df['cluster_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 0.5598479992426193\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on training data\n",
    "predicted_mnb_stemmed_train = text_mnb_stemmed.predict(tweets_df['text'])\n",
    "print('The accuracy on training data is', np.mean(predicted_mnb_stemmed_train == tweets_df['cluster_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the cluster label on test dataset\n",
    "test_df = test_tweets_df\n",
    "test_df['predicted_cluster_label'] = ''\n",
    "test_df['predicted_cluster_label'] = text_mnb_stemmed.predict(test_df['text'])\n",
    "\n",
    "# Cordinate prediction of the test text. Mapping the cluster label to the cluster center to assign the latitude and longitude to the text\n",
    "test_df['predicted_long'] = test_df['predicted_cluster_label'].apply(lambda l: cluster_centers[int(l)][1])\n",
    "test_df['predicted_lat'] = test_df['predicted_cluster_label'].apply(lambda l: cluster_centers[int(l)][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# City prediction. Mapping the most commonly occuring city label in the cluster to the text\n",
    "def predict_most_common_city_by_cluster():\n",
    "    cluster_city = dict()    # Most frequently occurring city for each cluster\n",
    "\n",
    "    for cluster_label, df in tweets_df.groupby('cluster_label'):\n",
    "        cluster_city[cluster_label] = df['tweet_city'].mode()[0]\n",
    "\n",
    "    return cluster_city\n",
    "cluster_city = predict_most_common_city_by_cluster()\n",
    "test_df['predicted_city'] = test_df['predicted_cluster_label']\n",
    "test_df['predicted_city'] = test_df['predicted_city'].map(cluster_city)\n",
    "\n",
    "#print('Results for city prediction by coordinates')\n",
    "#report_result(test_labels, test_df['predicted_city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging the prediction and oracle data in formats compatible with the evaluation script.\n",
    "test_df['id'] = test_df.index\n",
    "\n",
    "output_df = test_df[['id', 'predicted_city', 'predicted_lat', 'predicted_long']]\n",
    "oracle_df = test_df[['id', 'tweet_city', 'coordinates_lat', 'coordinates_long']]\n",
    "\n",
    "output_data = list(output_df.to_records(index=False))\n",
    "oracle_data = list(oracle_df.to_records(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating result\n",
      "WNUT evaluation: , , 0.1247, 1324.2263, 4067.1548\n"
     ]
    }
   ],
   "source": [
    "# Evaluation WNUT\n",
    "\n",
    "# !{sys.executable} -m pip install ujson\n",
    "\n",
    "# import ujson as json\n",
    "import math\n",
    "\n",
    "EARTH_RADIUS = 6372.8\n",
    "\n",
    "\n",
    "def _calc_dist_radian(pLat, pLon, lat, lon):\n",
    "    \"\"\"\n",
    "    Calculate the Great Circle Distance between two points on earth\n",
    "    http://en.wikipedia.org/wiki/Great-circle_distance\n",
    "    \"\"\"\n",
    "    cos_pLat = math.cos(pLat)\n",
    "    sin_pLat = math.sin(pLat)\n",
    "    cos_lat = math.cos(lat)\n",
    "    sin_lat = math.sin(lat)\n",
    "    long_diff = pLon - lon\n",
    "    cos_long_diff = math.cos(long_diff)\n",
    "    sin_long_diff = math.sin(long_diff)\n",
    "    numerator = math.sqrt(math.pow(cos_lat * sin_long_diff, 2) +\n",
    "                          math.pow(cos_pLat * sin_lat - sin_pLat * cos_lat * cos_long_diff, 2))\n",
    "    denominator = sin_pLat * sin_lat + cos_pLat * cos_lat * cos_long_diff\n",
    "    radian = math.atan2(numerator, denominator)\n",
    "    return radian * EARTH_RADIUS\n",
    "\n",
    "\n",
    "def _degree_radian(degree):\n",
    "    return (degree * math.pi) / 180\n",
    "\n",
    "\n",
    "def calc_dist_degree(pLat, pLon, lat, lon):\n",
    "    pLat = _degree_radian(pLat)\n",
    "    pLon = _degree_radian(pLon)\n",
    "    lat = _degree_radian(lat)\n",
    "    lon = _degree_radian(lon)\n",
    "    return _calc_dist_radian(pLat, pLon, lat, lon)\n",
    "\n",
    "\n",
    "def evaluate_submission(output_data, oracle_data):\n",
    "#     output_data = etl_data(output_file, submission_type)\n",
    "#     oracle_data = etl_data(oracle_file, submission_type)\n",
    "    assert len(output_data) == len(oracle_data)\n",
    "    accuracy, median_error, mean_error = 0.0, 0.0, 0.0\n",
    "    right, wrong = 0, 0\n",
    "    error_distance_list = []\n",
    "    for output, oracle in zip(output_data, oracle_data):        \n",
    "        assert output[0] == oracle[0]\n",
    "        if output[1] == oracle[1]:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        error_distance = calc_dist_degree(output[2], output[3], oracle[2], oracle[3])\n",
    "        error_distance_list.append(error_distance)\n",
    "\n",
    "    accuracy = round(right / (right + wrong + 1e-6), 4)\n",
    "    error_distance_list.sort()\n",
    "    total_num = len(error_distance_list)\n",
    "        \n",
    "    mean_error = round(sum(error_distance_list) / (total_num + 1e-6), 4)\n",
    "    median_error = round(error_distance_list[int(total_num / 2)], 4)\n",
    "    \n",
    "    \n",
    "    output_file = ''\n",
    "    submission_type = ''\n",
    "    \n",
    "    result = \"WNUT evaluation: {}, {}, {}, {}, {}\".format(output_file,\n",
    "                                                  submission_type,\n",
    "                                                  accuracy,\n",
    "                                                  median_error,\n",
    "                                                  mean_error)\n",
    "    print(result)\n",
    "    \n",
    "\n",
    "    \n",
    "print('Evaluating result')\n",
    "evaluate_submission(output_data, oracle_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print('Finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
