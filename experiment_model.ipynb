{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "FILE_TEST = 'formatted_csv_dataset/formatted_test.tweet.csv'\n",
    "FILE_TRAIN = 'formatted_csv_dataset/formatted_training.csv'\n",
    "\n",
    "test_tweets_df = pd.read_csv(FILE_TEST, sep=',')\n",
    "test_tweets_df = test_tweets_df[['text', 'coordinates_lat', 'coordinates_long', 'tweet_city']]    # Keep only relevant columns\n",
    "\n",
    "tweets_df = pd.read_csv(FILE_TRAIN, sep=',')\n",
    "\n",
    "# Swap latitude and longitude column values; we made a mistake when formatting the dataset to a csv file\n",
    "tweets_df.rename(columns={'coordinates_lat' : 'coordinates_lat_old'}, inplace=True)\n",
    "tweets_df.rename(columns={'coordinates_long' : 'coordinates_lat'}, inplace=True)\n",
    "tweets_df.rename(columns={'coordinates_lat_old' : 'coordinates_long'}, inplace=True)\n",
    "\n",
    "tweets_df = tweets_df[['id', 'text', 'coordinates_lat', 'coordinates_long', 'tweet_city']]    # Keep only relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preprocess(tweet):\n",
    "    if not tweet:\n",
    "        return ''\n",
    "    try:\n",
    "        tweet = re.sub(r'http\\S+', '', tweet)    # Remove urls\n",
    "        tokenizer = nltk.RegexpTokenizer(r'\\w+')    # Remove special characters\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "    except:\n",
    "        print('error: {}'.format(tweet))\n",
    "        return ''\n",
    "\n",
    "    if len(tokens) < 3:    # Ignore tweets with less than 3 words  \n",
    "        return ''\n",
    "        \n",
    "    return ' '.join(tokens)\n",
    " \n",
    "\n",
    "def get_preprocessed_df(df):\n",
    "    df['text'] = df['text'].apply(lambda text: preprocess(text))   \n",
    "    df = df.loc[df['text'].apply(lambda t: t.strip() != '')]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "print('Preprocessing text')\n",
    "tweets_df = get_preprocessed_df(tweets_df)\n",
    "test_tweets_df = get_preprocessed_df(test_tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with 3 means\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webis/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Creating labels for classification using clustering (manually setting num_clusters)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "NUM_CLUSTERS = 10\n",
    "coordinates_arr = tweets_df.as_matrix(columns=['coordinates_long', 'coordinates_lat'])\n",
    "\n",
    "print('Clustering with {} means'.format(NUM_CLUSTERS))\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, init='k-means++')\n",
    "tweets_df['cluster_label'] = kmeans.fit_predict(coordinates_arr)\n",
    "cluster_centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_EMBEDDINGS = False    # Configure word embeddings based features or tf-idf based    \n",
    "NUM_FEATURES = 300\n",
    "\n",
    "\n",
    "def get_tweet_vector(tweet_row, model, vocab):\n",
    "    # Get vector for tweet by computing average for all tokens weighted by their tf-idf values\n",
    "    \n",
    "    num_tokens = 1\n",
    "    tweet_vec = np.zeros(NUM_FEATURES)\n",
    "    \n",
    "    for i in range(len(vocab)):\n",
    "        token = vocab[i]\n",
    "        token_val = tweet_row[i]    # tf-idf value for feature corresponding to the token\n",
    "        \n",
    "        if token not in model.vocab or token_val == 0.0:\n",
    "            continue\n",
    "            \n",
    "        token_vec = model.get_vector(token)         \n",
    "        tweet_vec += token_val * token_vec\n",
    "        num_tokens += 1\n",
    "        \n",
    "    return tweet_vec / num_tokens\n",
    "            \n",
    "                    \n",
    "        \n",
    "def get_embedding_features(tf_idf_features, model, vocab):\n",
    "    # Get tweet vector for each tweet in the dataset\n",
    "    \n",
    "    embed_features = list()\n",
    "    \n",
    "    for tweet_row in tf_idf_features:\n",
    "        embed_features.append(get_tweet_vector(tweet_row, model, vocab))\n",
    "        \n",
    "    return np.array(embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = None\n",
    "\n",
    "if USE_EMBEDDINGS:\n",
    "    print('Loading embedding model')\n",
    "    word_embeddings_file = '../../../text_rewriting/arnet_miner_analysis/wiki.en.vec'\n",
    "    model = KeyedVectors.load_word2vec_format(word_embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features\n",
      "Training and predicting lat and long using classifier: LogisticRegression()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def report_result(test_labels, predicted_labels):\n",
    "    print('Accuracy: ', accuracy_score(y_true=test_labels, y_pred=predicted_labels))\n",
    "    print('Macro f1 score: ', f1_score(y_true=test_labels, y_pred=predicted_labels, average='macro'))\n",
    "    print('Micro f1 score: ', f1_score(y_true=test_labels, y_pred=predicted_labels, average='micro'))\n",
    "\n",
    "\n",
    "TRAIN_SIZE = 100000    # We experiment 100k data points due to memory limitations\n",
    "\n",
    "train_df = tweets_df.head(TRAIN_SIZE)\n",
    "test_df = test_tweets_df\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')    # Stop words filtered\n",
    "\n",
    "print('Extracting tf-idf features')\n",
    "train_features = vectorizer.fit_transform(train_df['text']).toarray()    \n",
    "test_features = vectorizer.transform(test_df['text']).toarray()\n",
    "\n",
    "if USE_EMBEDDINGS:\n",
    "    print('Computing word vector based features')    \n",
    "    vocab = vectorizer.get_feature_names()    \n",
    "    assert len(vocab) == len(train_features[0])\n",
    "    assert len(vocab) == len(test_features[0])    \n",
    "    train_tf_idf_shape = train_features.shape\n",
    "    test_tf_idf_shape = test_features.shape\n",
    "    \n",
    "    train_features = get_embedding_features(train_features, model, vocab)\n",
    "    test_features = get_embedding_features(test_features, model, vocab)\n",
    "    \n",
    "    assert train_features.shape[0] == train_tf_idf_shape[0]\n",
    "    assert test_features.shape[0] == test_tf_idf_shape[0]\n",
    "    \n",
    "\n",
    "# Predicting latitude and longitude\n",
    "\n",
    "train_labels = train_df['cluster_label']\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# classifier = GaussianNB()\n",
    "\n",
    "# classifier = SVC()\n",
    "\n",
    "print('Training and predicting lat and long using classifier: {}'.format(classifier))\n",
    "classifier.fit(train_features, train_labels)\n",
    "test_df['predicted_cluster_label'] = classifier.predict(test_features)\n",
    "\n",
    "test_df['predicted_long'] = test_df['predicted_cluster_label'].apply(lambda l: cluster_centers[int(l)][0])\n",
    "test_df['predicted_lat'] = test_df['predicted_cluster_label'].apply(lambda l: cluster_centers[int(l)][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted labels for tweets so we don't have to cluster and train again\n",
    "\n",
    "# SAVE_PREDICTED_DFS = False    # Change this during runtime\n",
    "\n",
    "# if SAVE_PREDICTED_DFS:\n",
    "#     try:\n",
    "#         save_csv_file_name = 'test_{}_clusters_nb_embedding_{}.csv'.format(NUM_CLUSTERS, USE_EMBEDDINGS)\n",
    "#         test_df.to_csv('dataset_with_predictions/' + save_csv_file_name, index=False)\n",
    "\n",
    "#         save_csv_file_name = 'train_{}_clusters_nb_embedding_{}.csv'.format(NUM_CLUSTERS, USE_EMBEDDINGS)\n",
    "#         train_df.to_csv('dataset_with_predictions/' + save_csv_file_name, index=False)\n",
    "\n",
    "#         print('Saved predicted dataset files')\n",
    "#     except Exception as e:\n",
    "#         print('Encountered exception {} when saving df files'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['tweet_city']\n",
    "test_labels = test_df['tweet_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_most_common_city_by_cluster():\n",
    "    cluster_city = dict()    # Most frequently occurring city for each cluster\n",
    "\n",
    "    for cluster_label, df in train_df.groupby('cluster_label'):\n",
    "        most_freq_city = df['tweet_city'].mode()[0]\n",
    "        cluster_city[cluster_label] = most_freq_city\n",
    "    \n",
    "    try:\n",
    "        test_df['predicted_city'] = test_df['predicted_cluster_label'].apply(lambda l: cluster_city[l])    \n",
    "    except Exception as e:\n",
    "        print('Encountered error, predicting most common city: {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for city prediction by coordinates\n",
      "Accuracy:  0.0\n",
      "Macro f1 score:  0.0\n",
      "Micro f1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "predict_most_common_city_by_cluster()\n",
    "\n",
    "print('Results for city prediction by coordinates')\n",
    "report_result(test_labels, test_df['predicted_city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['id'] = test_df.index\n",
    "\n",
    "output_df = test_df[['id', 'predicted_city', 'predicted_lat', 'predicted_long']]\n",
    "oracle_df = test_df[['id', 'tweet_city', 'coordinates_lat', 'coordinates_long']]\n",
    "\n",
    "output_data = list(output_df.to_records(index=False))\n",
    "oracle_data = list(oracle_df.to_records(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating result\n",
      "WNUT evaluation: , , 0.0, 8758.0217, 8669.2916\n"
     ]
    }
   ],
   "source": [
    "# Official evaluation for the shared task - WNUT\n",
    "\n",
    "# !{sys.executable} -m pip install ujson\n",
    "\n",
    "# import ujson as json\n",
    "import math\n",
    "\n",
    "EARTH_RADIUS = 6372.8\n",
    "\n",
    "\n",
    "def _calc_dist_radian(pLat, pLon, lat, lon):\n",
    "    \"\"\"\n",
    "    Calculate the Great Circle Distance between two points on earth\n",
    "    http://en.wikipedia.org/wiki/Great-circle_distance\n",
    "    \"\"\"\n",
    "    cos_pLat = math.cos(pLat)\n",
    "    sin_pLat = math.sin(pLat)\n",
    "    cos_lat = math.cos(lat)\n",
    "    sin_lat = math.sin(lat)\n",
    "    long_diff = pLon - lon\n",
    "    cos_long_diff = math.cos(long_diff)\n",
    "    sin_long_diff = math.sin(long_diff)\n",
    "    numerator = math.sqrt(math.pow(cos_lat * sin_long_diff, 2) +\n",
    "                          math.pow(cos_pLat * sin_lat - sin_pLat * cos_lat * cos_long_diff, 2))\n",
    "    denominator = sin_pLat * sin_lat + cos_pLat * cos_lat * cos_long_diff\n",
    "    radian = math.atan2(numerator, denominator)\n",
    "    return radian * EARTH_RADIUS\n",
    "\n",
    "\n",
    "def _degree_radian(degree):\n",
    "    return (degree * math.pi) / 180\n",
    "\n",
    "\n",
    "def calc_dist_degree(pLat, pLon, lat, lon):\n",
    "    pLat = _degree_radian(pLat)\n",
    "    pLon = _degree_radian(pLon)\n",
    "    lat = _degree_radian(lat)\n",
    "    lon = _degree_radian(lon)\n",
    "    return _calc_dist_radian(pLat, pLon, lat, lon)\n",
    "\n",
    "\n",
    "def evaluate_submission(output_data, oracle_data):\n",
    "#     output_data = etl_data(output_file, submission_type)\n",
    "#     oracle_data = etl_data(oracle_file, submission_type)\n",
    "    assert len(output_data) == len(oracle_data)\n",
    "    accuracy, median_error, mean_error = 0.0, 0.0, 0.0\n",
    "    right, wrong = 0, 0\n",
    "    error_distance_list = []\n",
    "    for output, oracle in zip(output_data, oracle_data):        \n",
    "        assert output[0] == oracle[0]\n",
    "        if output[1] == oracle[1]:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "        error_distance = calc_dist_degree(output[2], output[3], oracle[2], oracle[3])\n",
    "        error_distance_list.append(error_distance)\n",
    "\n",
    "    accuracy = round(right / (right + wrong + 1e-6), 4)\n",
    "    error_distance_list.sort()\n",
    "    total_num = len(error_distance_list)\n",
    "        \n",
    "    mean_error = round(sum(error_distance_list) / (total_num + 1e-6), 4)\n",
    "    median_error = round(error_distance_list[int(total_num / 2)], 4)\n",
    "    \n",
    "    \n",
    "    output_file = ''\n",
    "    submission_type = ''\n",
    "    \n",
    "    result = \"WNUT evaluation: {}, {}, {}, {}, {}\".format(output_file,\n",
    "                                                  submission_type,\n",
    "                                                  accuracy,\n",
    "                                                  median_error,\n",
    "                                                  mean_error)\n",
    "    print(result)\n",
    "    \n",
    "\n",
    "    \n",
    "print('Evaluating result')\n",
    "evaluate_submission(output_data, oracle_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "print('Finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
